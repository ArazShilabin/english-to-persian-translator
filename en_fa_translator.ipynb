{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArazShilabin/english-to-persian-translator/blob/main/en_fa_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgriPT0wETsq"
      },
      "source": [
        "### mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lzcb1Hu3C9Sh",
        "outputId": "1fb98c55-32fd-4aa8-d0d9-6f54117ca8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/projects/persian_english_translator'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "\"\"\" \n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\n",
        "\n",
        "########################\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "########################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqojleITLGJS"
      },
      "source": [
        "### change current path to where the working project folder is at"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmCJ8OpxKDuf",
        "outputId": "a50724ae-e6bd-460f-d21d-ca6180234fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/projects/persian_english_translator/'\n",
            "/content/drive/MyDrive/projects/persian_english_translator\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/projects/persian_english_translator/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoTBod--M9Ci"
      },
      "source": [
        "# Step 0: Get The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvWJe_YqK_kJ"
      },
      "source": [
        "### upload the data to our current path and unzip it (uncomment and run this only once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ3dNq5-jZ_h"
      },
      "outputs": [],
      "source": [
        "# # persian data\n",
        "# %mkdir -p data  # make dir if doesn't exist\n",
        "# %cd data\n",
        "# !wget https://github.com/omidkashefi/Mizan/raw/master/mizan.zip\n",
        "# !unzip mizan.zip\n",
        "# %cd ..\n",
        "# %pwd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bJw45b2BWqx"
      },
      "source": [
        "### Get non breaking prefixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI3vtoy1BXOh"
      },
      "outputs": [],
      "source": [
        "# get non_breaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes\n",
        "# then rename them to: \"nonbreaking_prefix.en\" and \"nonbreaking_prefix.de\" and put them in your data folder so we dont consider the\n",
        "# dot in 'mr.jackson' as the end of a sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBnbCu6GHbMJ"
      },
      "source": [
        "# Step 1: Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjF371EVHM6n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time # to see how long it takes in training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwAD6AfAJSWp"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds # tools for the tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovVn780GHrrc"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiC7Z0ACNWjd"
      },
      "source": [
        "## read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My08LADJHrFo",
        "outputId": "3dabb338-fd29-49f7-9cdb-f4b550f0b072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The story which follows was first written out in P\n",
            "داستانی که از نظر شما می‌گذرد، ابتدا ضمن کنفرانس ص\n"
          ]
        }
      ],
      "source": [
        "with open(\"data/mizan/mizan_en.txt\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"data/mizan/mizan_fa.txt\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_fa = f.read()\n",
        "\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_fa[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mFVDgeZOzrU"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkWn8E4cvoVM"
      },
      "source": [
        "### English: <br> 1. All lowercase <br> 2. Split by \"\\\\n\" to a list of sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgcNPbA8SE9b",
        "outputId": "0c886aed-144d-452f-92a2-0301f9156897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1021596\n",
            "afterwards in the autumn of this first draft and some of the notes were lost\n"
          ]
        }
      ],
      "source": [
        "text_en = text_en.lower()\n",
        "text_en = text_en.split(\"\\n\")\n",
        "text_en = [re.sub(r\"[^a-zA-Z\\n]\",\" \", i) for i in text_en]\n",
        "text_en = [re.sub(' +', ' ', i).strip() for i in text_en]\n",
        "\n",
        "print(len(text_en))\n",
        "print(text_en[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auOsIDrNqekw"
      },
      "source": [
        "### Farsi: <br> 1. Turning half-spaces to full-spaces <br> 2. Split by \"\\\\n\" to a list of sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1DaVuVPqEOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfc57d2-b9f9-4753-9e72-e3624e390f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1021596\n",
            "بعدا در پائیز سال این نوشته اولیه و بعضی از یادداشت ها مفقود شدند\n"
          ]
        }
      ],
      "source": [
        "text_fa = text_fa.replace(\"\\u200c\", \" \")\n",
        "text_fa = text_fa.split(\"\\n\")\n",
        "text_fa = [re.sub(r\"[^آ-ی]\",\" \", i) for i in text_fa]\n",
        "text_fa = [re.sub(' +', ' ', i).strip() for i in text_fa]\n",
        "\n",
        "print(len(text_fa))\n",
        "print(text_fa[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train/Test Split"
      ],
      "metadata": {
        "id": "9GpA3-OnZ1ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "text_en, text_en_test, text_fa, text_fa_test = train_test_split(text_en, text_fa, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "ySNVy4oFZy2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wmloUWExi_V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R96V-csIdC1p"
      },
      "source": [
        "### Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzNjGhLKT8OR",
        "outputId": "73e7a9c0-c624-407b-bb3f-48f3895c6115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done EN\n",
            "done FA\n"
          ]
        }
      ],
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000)\n",
        "print(\"done EN\")\n",
        "\n",
        "tokenizer_fa = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_fa, target_vocab_size=8000)\n",
        "print(\"done FA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z5zOdOEgvVm"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_FA = tokenizer_fa.vocab_size + 2\n",
        "\n",
        "# we put start and end tokens as size-1 and size-2 which are the same as\n",
        "# tokenizer_size and tokenizer_size+1 because the words are from [0 to ts-1]\n",
        "# tokenize_en.encode(sentence) give a list then list + list + list appends them\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in text_en]\n",
        "outputs = [[VOCAB_SIZE_FA-2] + tokenizer_fa.encode(sentence) + [VOCAB_SIZE_FA-1]\n",
        "          for sentence in text_fa]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONDBNqJsIqU"
      },
      "source": [
        "### Remove too long sentences\n",
        "Why?<br>\n",
        "(1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100<br>\n",
        "(2) takes too much time to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRoMWeU7r-tD"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20\n",
        "\n",
        "# this part, why we do it is a bit tricky, pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "# we remove in reversed because of shifting issuies when we start from begining\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs>20\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wvxrND85t7r"
      },
      "source": [
        "### input/output creation\n",
        "1) padding  \n",
        "2) batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47hjlJ1SvXwK"
      },
      "outputs": [],
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVq4wN816gOj"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "# this is something that improves the way the dataset is stored, it increases\n",
        "# the speed of accessing the data which increases training speed in return:\n",
        "dataset = dataset.cache()\n",
        "\n",
        "# now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "# this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoOMHhKU8kFo"
      },
      "source": [
        "# Step 3: Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alCnJ2ZtTNJt"
      },
      "source": [
        "## A - Positional Encoding (look at the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoCaYr408as-"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # this Positional Encoder we made it a child of the Layers so it has all\n",
        "        # the properties that a layer has\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) index of the word in sentence [0 to 19]\n",
        "        :i: the dimensions of the embedding (glove dims 200) then-> [0 to 199]\n",
        "        :d_model: the size (dimension) of the embeded (e.g. glove size 200)\n",
        "        :return: (seq_len, d_model) why? we are getting the encoding of the\n",
        "                every positions vs every one of the dimensions of that word\n",
        "        \"\"\"\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "        # keep in mind we DONT change the values of the input considering \n",
        "        # their positions, we just get the dims from input and calculate\n",
        "        # pos encoding totally seperatly and stack them at the end\n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # we do this because it has a [batch] dimension at the begining we add\n",
        "        # it. why? because inputs and the encodings need to be same dims so we\n",
        "        # make newaxis which it doesnt put 0's.... it copies those same dims for\n",
        "        # all the batches...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # now we need to return both the inputs and their pos_encodings\n",
        "        # but we have pos_encoding in np so we make them tf\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BEvzpOwa-RD"
      },
      "source": [
        "## B - Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8AbGEHbFua"
      },
      "source": [
        "### Attention computation (see the formula in the paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4SX3F__VjXc"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K will be [output_len, d_model] * [d_model, input_len] which both are 20\n",
        "    # for both english and french\n",
        "    # the transpose_b=True makes keys turn to keys.T\n",
        "    # each of them are this dim: [batch_size, nb_proj, seq_len, d_proj]\n",
        "    # so with transpose it become: [a,b,c,d] * []\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scales it (formula stuff)\n",
        "\n",
        "    # because this mask as the paper said is optional to prevent the program\n",
        "    # from seeing the feauture. why? because when we backprop then they will\n",
        "    # consider the stuff in front of them so to stop this we add -1e9 to them\n",
        "    # so after softmax the probabilities become 0 for them\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    # we apply the softmax along the last axis because we want their sum to be 1\n",
        "    # scaled_product = [output_len, input_len] -> softmax on input_len so\n",
        "    # basically we are keeping in_len the same but finding the probs for out_len\n",
        "    # so for every ins what is the prob of each of the outs\n",
        "    # (e.g. ith input, the probs [0.3,0.7] of the outs)\n",
        "    probs = tf.nn.softmax(scaled_product, axis=-1)\n",
        "\n",
        "    # attention = [output_len, input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # so now we have d_model weights for each of the output words which we will\n",
        "    # feed to forwards to see their prediction for each of the out_lens\n",
        "    attention = tf.matmul(probs, values)\n",
        "\n",
        "    return attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7xS01sz8Yzn"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# # this is just a test for you to see what happens in this line of code to the\n",
        "# # dims (which we realized from the 4 dims, it only transposed the last two and\n",
        "# # did mult only on those last two because matmul considers the other dims as\n",
        "# # batch size and other stuff) (tf.matmul(a, b, transpose_b=True))\n",
        "# a = np.arange(24).reshape(1,2,3,4)\n",
        "# a = tf.convert_to_tensor(a, np.float32)\n",
        "# b = np.arange(24).reshape(1,2,3,4)\n",
        "# b = tf.convert_to_tensor(b, np.float32)\n",
        "# product = tf.matmul(a, b, transpose_b=True)\n",
        "# print(product.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTfOzkdMbK2j"
      },
      "source": [
        "### Multi-Head attention sublayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzN_spFEbS5K"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj: the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "    \n",
        "    # this is the same as init but it happens when we USE the object for the\n",
        "    # first time, in init it was called when we CREATED the object\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we wanna make sure they are divisible\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        # we use 2 slashes to make it integer\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "    \n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        :inputs: [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        :return: \n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj here is like channels in cnn\n",
        "            we basically split the d_model to nb_proj*d_proj so d_proj is\n",
        "            found by doing d_model/nb_proj\n",
        "        \"\"\"\n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        # here we will get: [batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to: [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "\n",
        "        # now we split each of them to make projs\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        # each of the q,k,v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        # now we will reverse the splits we did above: reshape + concat\n",
        "        attention = tf.transpose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2, 3\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N7jK4SvFZEb"
      },
      "source": [
        "## C - Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vEIb2g_5igO"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units:\n",
        "            feed forward networks units: the number of units for the\n",
        "            feed forward which you can see in the encoder part of the\n",
        "            paper (right after the attention there is a feed forward...)\n",
        "        :nb_project: \n",
        "            the number of projections we have (8)\n",
        "        :dropout:\n",
        "            the dropout rate e.g. 0.3\n",
        "        \"\"\"\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "    # we use this because we dont have many of the vars we want\n",
        "    # when we create the Encoder, so no we can get them when we use the\n",
        "    # function using 'build' instead\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # we first build the object for the multi-head-attention\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        :mask: which we will apply in the multi-head attention\n",
        "        :training: \n",
        "            it is true/false which we use dropout while we train=true to stop\n",
        "            the model from overfiting but we dont use it when we are just\n",
        "            testing (aka. train=false)\n",
        "        \"\"\"\n",
        "        # if you look at the architecture you see that in the encoder\n",
        "        # all of the query/key/val are the same array which is the input we\n",
        "        # got from the previous layer\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        # we do + inputs here because in the architecture they still concat the\n",
        "        # previous inputs to our resulted attention then we normalize it\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABusZar2NKT6"
      },
      "outputs": [],
      "source": [
        "class Encoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name=name here because the name is something that belongs\n",
        "        # to the layers class, so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(self.nb_encoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        \n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaf9z1oLXgjx"
      },
      "source": [
        "## D - Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXEa6VtkWP4b"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "        # MHA 2\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # FFN\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # check the architecture in the paper to see why we do these\n",
        "\n",
        "        # this is the 1# attention\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention, this is ALOT different than before one\n",
        "        # pay attention to it's inputs\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                enc_outputs,\n",
        "                                                enc_outputs,\n",
        "                                                mask_2)\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLUUmK6hfoj7"
      },
      "outputs": [],
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row\n",
        "        self.d_model = d_model # the size of the output e.g. glove(200)\n",
        "\n",
        "        # we give vocab size for it to know the maximum number used in vocab\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout)\n",
        "                        for _ in range(nb_decoding_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # look at the paper's architecture while doing these\n",
        "        # embedding with maybe glove weights....\n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was because of what was writtent\n",
        "        # on the paper in secssion 3.4 which they said they multiplied\n",
        "        # it by sqrt of d_model\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        # this will give us the concat: outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        # now we do dropout before all the encoding layers\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        # now we do the EmbeddingLayer a couple of times, not just once.\n",
        "        for i in range(self.nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                             enc_outputs,\n",
        "                                             mask_1,\n",
        "                                             mask_2,\n",
        "                                             training)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQz4ZoHhm1v"
      },
      "source": [
        "## E - Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "begalnPMhXVT"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        # initing the Objects\n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        # this is at the very end after you combined the enc & dec the output\n",
        "        # will of size vocab_dec\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec)\n",
        "        \n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # so this gives us element wise equal true/false with broadcasting on 0\n",
        "        # so we just want to see which words dont exist to give it true in all\n",
        "        # the batches\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        # so now we will return that mask we made but with 2 broadcasted new \n",
        "        # dimensions so it can match the input needed in attention\n",
        "        # (in the next cell I made and example to see it better)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        \"\"\"\n",
        "        :seq: [batch_size, seq_len(20)]\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        # sample of what it produces is in the cell below\n",
        "        # why do we do this? because when we predict the ith word we dont see\n",
        "        # words from\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # combining the encoder and decoder and masks here\n",
        "\n",
        "        # creating the mask for encoder\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        # creating the mask for decoder\n",
        "\n",
        "        # mask #1 is for the first decoder attention which uses the\n",
        "        # output, output, output as q/k/v so we get max of the 2 masks for it\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                self.create_look_ahead_mask(dec_inputs))\n",
        "        # mask #2 is for the second decoder attention in which we use the\n",
        "        # output of the encoder as v/k so we need to do masking on the input\n",
        "        # so that later when doing q*k then *v we can get a correct output\n",
        "        # this is what the video said, but i belive making this None is alot                  # try making this none later\n",
        "        # more correct since we dont actually use the inputs and outputs but\n",
        "        # their already masked and processed outputs from previous attentions\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSRSyG4XwOp0"
      },
      "source": [
        "### testing masks to see their outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMj-AkOvn-Tl"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL6GXKykpln1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e24a72-2a38-4efc-cf52-b70214a593bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj\n",
            "tf.Tensor([[[[0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 1, 8), dtype=float32)\n",
            "\n",
            "This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\n",
            "Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0\n",
            "tf.Tensor(\n",
            "[[0. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
            "\n",
            "Now applying both: (pay very close attention to this samples output, very important)\n",
            "tf.Tensor(\n",
            "[[[[0. 1. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 1. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]\n",
            "   [0. 0. 0. 1. 0. 1. 1. 1.]]]], shape=(1, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# sample: 1 batch of seq_len=8 ->[1,8] this will become [1,1,1,8] then\n",
        "# broadcasting will happen for later stages\n",
        "# mask=1 means we should delete this\n",
        "seq = tf.cast([[1, 2, 3, 0, 4, 0, 0, 0]], tf.int32)\n",
        "\n",
        "print('This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj')\n",
        "print(create_padding_mask(seq), end='\\n\\n')\n",
        "\n",
        "print(\"This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\")\n",
        "print('Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0')\n",
        "print(1 - tf.linalg.band_part(tf.ones((5, 5)), -1, 0), end='\\n\\n')\n",
        "\n",
        "print('Now applying both: (pay very close attention to this samples output, very important)')\n",
        "print(tf.maximum(create_padding_mask(seq),\n",
        "                 create_look_ahead_mask(seq)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzvSJakz0yUJ"
      },
      "source": [
        "# Step 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhfYzD5Oz7cg"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUemDV6BqMTs"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters:\n",
        "EPOCHS = 1\n",
        "D_MODEL = 128 # 512 takes more time but has a lot better results\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FA,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woJ2_wR5kc7h"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zLjSgOtVCAR"
      },
      "outputs": [],
      "source": [
        "def custom_sparse_categorical_accuracy(y_true, y_pred):\n",
        "    return K.cast(K.equal(K.max(y_true, axis=-1),\n",
        "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n",
        "                  K.floatx())\n",
        "    \n",
        "def sparse_cross_entropy(y_true, y_pred):\n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                          logits=y_pred)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OJChh-xscqd"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "def loss_function(target,prediction):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target,0))\n",
        "\n",
        "    loss_ = loss_object(target,prediction)\n",
        "    \n",
        "    mask = tf.cast(mask,dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z_7kqLdkkFB"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL71RRe3kY3Y"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    # take notice that we used __call__ instead of call\n",
        "    # and that the parameter steps we dont give it, we get it\n",
        "    # from the tf.keras.optimizers.schedules.LearningRateSchedule itself\n",
        "    def __call__(self, step):\n",
        "        # read this part in paper and you'll understand arg1 & arg2 which they\n",
        "        # used in their custom learning rate\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TNz_bWzr1cI"
      },
      "source": [
        "## Checkpoints (delete your checkpoints if there are some unexpected errors when changing your data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9v_ZbpFpxZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13e70b0c-bf21-4726-9b2c-722534d141eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest Checkpoint Restored...\n"
          ]
        }
      ],
      "source": [
        "# making a checkpoint:\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# lets check if we already have a checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84_3qydkr-4N"
      },
      "source": [
        "## Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S5Mfu-arnsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edcc7eed-9563-4662-c016-532fbe400a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 2.2562 Accuracy 0.1612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-41-8c2107a019cc>\", line 25, in <module>\n",
            "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\", line 1087, in gradient\n",
            "    unconnected_gradients=unconnected_gradients)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\", line 73, in imperative_grad\n",
            "    compat.as_str(unconnected_gradients.value))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\", line 156, in _gradient_function\n",
            "    return grad_fn(mock_op, *out_grads)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 1741, in _MatMulGrad\n",
            "    grad_b = gen_math_ops.mat_mul(a, grad, transpose_a=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 6015, in mat_mul\n",
            "    transpose_b)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # iterate on each batch:\n",
        "    for (batch_index, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        # we take all the target minus the last word: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # why? because we are trying to predict the next word each time, so at the last step\n",
        "        # we are predicting <e> and we are done, so we wont need it as an input for our decoder\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        # we shift 1 to right because tokens are: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # when we want to do the predictions, we wont need to predict the <s>, we start with <s>\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        # this will record everything that happens when we do predictions\n",
        "        with tf.GradientTape() as tape:\n",
        "            # the true is for training\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        # now we get the gradients using this method using the tape\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # now we apply the gradients according to our Adam optimizer\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        # now lets add our loss to the train loss object that keeps track of the loss\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        # now let's print our loss and acc from time to time.....\n",
        "        if batch_index % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch_index, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    \n",
        "    # at the end of each epoch we save a checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    # model.save(\"model.h5\")\n",
        "    print(\"Saved checkpoint for epoch {}!\".format(epoch+1))\n",
        "        \n",
        "# Epoch 3 Batch 0 Loss 2.2562 Accuracy 0.1612\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DboKBdVg48mc"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LceVAvXNv7pV"
      },
      "outputs": [],
      "source": [
        "def evaluate(inp_sentence):\n",
        "    # preprocessing\n",
        "    inp_sentence = inp_sentence.lower()\n",
        "    inp_sentence = re.sub(r\"[^a-zA-Z]\",\" \", inp_sentence)\n",
        "    inp_sentence = re.sub(' +', ' ', inp_sentence).strip()\n",
        "\n",
        "    # turn the sentence to the tokenizer_encoded format [hi, bye] -> [241, 6]\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # expand dim on axis=0 to simulate the batch dimmension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    # let's make the ouput which starts with <s> and add that axis=0 for batch=0\n",
        "    output = tf.expand_dims([VOCAB_SIZE_FA-2], axis=0)\n",
        "\n",
        "    # the loop to predict the next word of output each time and output += it\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # we put false because we are not training so no dropout\n",
        "        # predictions = [btch_sz=1, seq_len(output_so_far), vocav_sz_de(the \n",
        "        # softmax values of each word, the higher the number the higher the \n",
        "        # probability for that word)]\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        # we want to take the last word of this prediction\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # we do argmax to get the index of the most probable next word\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        # we reached the end of the sentence\n",
        "        if predicted_id == VOCAB_SIZE_FA-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        # now we know add the new prediction to the last of the output\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    #even if we didn't reach the end of the sentence we can't continue\n",
        "    return tf.squeeze(output, axis=0)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxXL5DaPPhKg"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    # get rid of <s> and <e> if they exist\n",
        "    output = [i for i in output if i < VOCAB_SIZE_FA-2]\n",
        "    # decode indexes to words e.g. [241, 6] -> [hi, bye] \n",
        "    predicted_sentence = tokenizer_fa.decode(output)\n",
        "    return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGOu9PO7Q797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b98491-a16b-40b2-d876-0b22de4ff3d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: I am happy\n",
            "Predicted translation: من خوشحالم\n"
          ]
        }
      ],
      "source": [
        "input = \"I am happy\"\n",
        "output = translate(input)\n",
        "print(\"Input: {}\".format(input))\n",
        "print(\"Predicted translation: {}\".format(output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNIHveQF7Bib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260b7ab5-76d7-4934-83d7-97b7eb7a8854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: what a nice day\n",
            "Predicted translation: چه روز خوبی است\n"
          ]
        }
      ],
      "source": [
        "input = \"what a nice day\"\n",
        "output = translate(input)\n",
        "print(\"Input: {}\".format(input))\n",
        "print(\"Predicted translation: {}\".format(output))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"How is this possible\"\n",
        "output = translate(input)\n",
        "print(\"Input: {}\".format(input))\n",
        "print(\"Predicted translation: {}\".format(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbPBXUZNdnn7",
        "outputId": "791cc172-a347-40b0-a6a4-153c67148bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: How is this possible\n",
            "Predicted translation: چه کار می تواند چه می شود\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"go on a trip\"\n",
        "output = translate(input)\n",
        "print(\"Input: {}\".format(input))\n",
        "print(\"Predicted translation: {}\".format(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8uOCt1OdxMa",
        "outputId": "8098be84-ca8c-4dab-84ef-37dce73f206f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: go on a trip\n",
            "Predicted translation: به راه بیفتید\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflowjs"
      ],
      "metadata": {
        "id": "N_5hxmDO8uXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !tensorflowjs_converter --input_format keras '/model.h5' '/model'"
      ],
      "metadata": {
        "id": "_9C5aSo48wBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bleu"
      ],
      "metadata": {
        "id": "Jcko9ZwPbN5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "check_n_first = 2000\n",
        "sum = 0\n",
        "for idx in range(len(text_en_test[:check_n_first])):\n",
        "    if idx % 50 == 0:\n",
        "        print(\"done: {} / {}\".format(idx, check_n_first))\n",
        "    hypothesis = translate(text_en_test[idx])\n",
        "    refrence = text_fa_test[idx]\n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu(refrence, hypothesis)\n",
        "    sum += BLEUscore\n",
        "print(sum / check_n_first)\n"
      ],
      "metadata": {
        "id": "Z6wEjzoN88-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92689fd8-954c-46de-83a4-5b2ec6f4abde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done: 0 / 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done: 50 / 2000\n",
            "done: 100 / 2000\n",
            "done: 150 / 2000\n",
            "done: 200 / 2000\n",
            "done: 250 / 2000\n",
            "done: 300 / 2000\n",
            "done: 350 / 2000\n",
            "done: 400 / 2000\n",
            "done: 450 / 2000\n",
            "done: 500 / 2000\n",
            "done: 550 / 2000\n",
            "done: 600 / 2000\n",
            "done: 650 / 2000\n",
            "done: 700 / 2000\n",
            "done: 750 / 2000\n",
            "done: 800 / 2000\n",
            "done: 850 / 2000\n",
            "done: 900 / 2000\n",
            "done: 950 / 2000\n",
            "done: 1000 / 2000\n",
            "done: 1050 / 2000\n",
            "done: 1100 / 2000\n",
            "done: 1150 / 2000\n",
            "done: 1200 / 2000\n",
            "done: 1250 / 2000\n",
            "done: 1300 / 2000\n",
            "done: 1350 / 2000\n",
            "done: 1400 / 2000\n",
            "done: 1450 / 2000\n",
            "done: 1500 / 2000\n",
            "done: 1550 / 2000\n",
            "done: 1600 / 2000\n",
            "done: 1650 / 2000\n",
            "done: 1700 / 2000\n",
            "done: 1750 / 2000\n",
            "done: 1800 / 2000\n",
            "done: 1850 / 2000\n",
            "done: 1900 / 2000\n",
            "done: 1950 / 2000\n",
            "0.7017855695792813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## match casing"
      ],
      "metadata": {
        "id": "c2fYtINmNgQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "check_n_first = 100\n",
        "sum = 0\n",
        "cnt = 0\n",
        "for idx in range(len(text_en_test[:check_n_first])):\n",
        "    if idx % 50 == 0:\n",
        "        print(\"done: {} / {}\".format(idx, check_n_first))\n",
        "    hypothesis = translate(text_en_test[idx])\n",
        "    refrence = text_fa_test[idx]\n",
        "    words = {}\n",
        "    word_in_ref = {}\n",
        "    for word in hypothesis:\n",
        "        if word in refrence:\n",
        "            word_in_ref[word] = 1\n",
        "        words[word] = 1\n",
        "    if len(words):\n",
        "        match_case = len(word_in_ref) / len(words)\n",
        "        sum += match_case\n",
        "        cnt += 1\n",
        "print(sum / cnt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsTNtZTzhe5p",
        "outputId": "aef994b5-a1c3-4e5c-ef27-b2d2b02b566c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done: 0 / 100\n",
            "done: 50 / 100\n",
            "0.8109631573706647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zJFFxRl1ON0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KqojleITLGJS",
        "HoTBod--M9Ci",
        "uvWJe_YqK_kJ",
        "2bJw45b2BWqx",
        "eBnbCu6GHbMJ",
        "QiC7Z0ACNWjd",
        "LkWn8E4cvoVM",
        "HONDBNqJsIqU",
        "4wvxrND85t7r",
        "VoOMHhKU8kFo",
        "alCnJ2ZtTNJt",
        "1BEvzpOwa-RD",
        "8N7jK4SvFZEb",
        "vaf9z1oLXgjx",
        "XjQz4ZoHhm1v",
        "woJ2_wR5kc7h",
        "2Z_7kqLdkkFB",
        "9TNz_bWzr1cI"
      ],
      "name": "en_fa_translator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaQXfVjuh0e93oRoAKJP1P",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}